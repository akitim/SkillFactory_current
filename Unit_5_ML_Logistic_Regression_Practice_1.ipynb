{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Unit_5_ML_Logistic_Regression_Practice_1.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akitim/SkillFactory_current/blob/main/Unit_5_ML_Logistic_Regression_Practice_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezfjQFtNAIXR"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.metrics import mean_squared_error, f1_score, accuracy_score, roc_curve, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfr63SJBAIXT"
      },
      "source": [
        "## 3. Логистическая регрессия. Реализация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xinPnVqTAIXX"
      },
      "source": [
        "Функция ошибки для логистической регрессии в случае бинарной классификации называется бинарной кросс-энтропией и записывается следующим образом:\n",
        "$$L=-\\frac{1}{n}(y_i \\log h_{\\theta}(x_i) + (1-y_i) \\log(1-h_{\\theta}(x_i))),$$\n",
        "где $x_i$ — вектор признаков $i$-го примера из обучающей выборки, $y_i$ — истинный класс для соответствующего примера (0 или 1), $n$ — число примеров в обучающей выборке, $h_{\\theta}(x)$ — sigmoid функция, равная:\n",
        "$$h_{\\theta}(x)=\\frac{1}{1+\\exp^{-\\theta x}},$$\n",
        "где $\\theta$ — вектор параметров логистической регрессии, $x$ - вектор признаков объекта из выборки."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xd3qB-5gAIXY"
      },
      "source": [
        "Соответствующий градиент функции ошибки равен:\n",
        "$$\\nabla L=\\frac{1}{n}\\sum_{i=1}^{n}{(h_{\\theta}(x_i)-y_i)x_i}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wv-oZPDUAIXY"
      },
      "source": [
        "Реализация логистической регрессии будет основана на оптимизации функции ошибки градиентным спуском."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KYx2RysAIXZ"
      },
      "source": [
        "В качестве экспериментальных данных возьмем датасет о доходах граждан в различных странах [Adult Income](https://archive.ics.uci.edu/ml/datasets/Adult) и сделаем необходимую предобработку."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYladtgoAIXZ"
      },
      "source": [
        "adult = pd.read_csv('./data/adult.data',\n",
        "                    names=['age', 'workclass', 'fnlwgt', 'education',\n",
        "                           'education-num', 'marital-status', 'occupation',\n",
        "                           'relationship', 'race', 'sex', 'capital-gain',\n",
        "                           'capital-loss', 'hours-per-week', 'native-country', 'salary'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQ4BD8ZsAIXb"
      },
      "source": [
        "# Описание датасета\n",
        "\n",
        "# with open('./data/adult.names', 'r') as f:\n",
        "#     names = f.read()\n",
        "# print(names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FO3GuoxWAIXb"
      },
      "source": [
        "adult.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn5o0Yn8AIXc"
      },
      "source": [
        "# Избавиться от лишних признаков\n",
        "adult.drop(['native-country'], axis=1, inplace=True)\n",
        "# Сконвертировать целевой столбец в бинарные значения\n",
        "adult['salary'] = (adult['salary'] != ' <=50K').astype('int32')\n",
        "# Сделать one-hot encoding для некоторых признаков\n",
        "adult = pd.get_dummies(adult, columns=['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0P_w3QtAIXc"
      },
      "source": [
        "adult.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oADN4pq9AIXd"
      },
      "source": [
        "# Нормализовать нуждающиеся в этом признаки\n",
        "a_features = adult[['age', 'education-num', 'hours-per-week', 'fnlwgt', 'capital-gain', 'capital-loss']].values\n",
        "norm_features = (a_features - a_features.mean(axis=0)) / a_features.std(axis=0)\n",
        "adult.loc[:, ['age', 'education-num', 'hours-per-week', 'fnlwgt', 'capital-gain', 'capital-loss']] = norm_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2Jw7GIKAIXe"
      },
      "source": [
        "adult.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYHqKwevAIXf"
      },
      "source": [
        "# Разбить таблицу данных на матрицы X и y\n",
        "X = adult[list(set(adult.columns) - set(['salary']))].values\n",
        "y = adult['salary'].values\n",
        "\n",
        "# Добавить фиктивный столбец единиц (bias линейной модели)\n",
        "X = np.hstack([np.ones(X.shape[0])[:, np.newaxis], X])\n",
        "m = X.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ilbrzB1AIXf"
      },
      "source": [
        "# Реализовать функцию sigmoid\n",
        "def sigmoid(X, theta):\n",
        "    return 1. / (1. + np.exp(-X.dot(theta)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdCxnx6OAIXg"
      },
      "source": [
        "# Реализовать функцию, вычисляющую градиент бинарной кросс-энтропии\n",
        "def calc_binary_cross_entropy_grad(X, y, theta):\n",
        "    n = X.shape[0]\n",
        "    grad = 1. / n * X.transpose().dot(sigmoid(X, theta) - y)\n",
        "    \n",
        "    return grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmzCOjxnAIXg"
      },
      "source": [
        "def gradient_step(theta, theta_grad, alpha):\n",
        "    return theta - alpha * theta_grad\n",
        "def optimize(X, y, grad_func, start_theta, alpha, n_iters):\n",
        "    theta = start_theta.copy()\n",
        "    \n",
        "    for i in range(n_iters):\n",
        "        theta_grad = grad_func(X, y, theta)\n",
        "        theta = gradient_step(theta, theta_grad, alpha)\n",
        "    \n",
        "    return theta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSMLHWR1AIXh"
      },
      "source": [
        "# Оптимизировать параметр линейной регрессии theta на всех данных\n",
        "theta = optimize(X, y, calc_binary_cross_entropy_grad, np.ones(m), 1., 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VA9p-JKxAIXh"
      },
      "source": [
        "theta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OCSRA0rAIXh"
      },
      "source": [
        "def print_logisitc_metrics(y_true, y_pred):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    print(f'acc = {acc:.2f} F1-score = {f1:.2f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdoILI_XAIXi"
      },
      "source": [
        "# Сделать предсказания на тренировочной выборке и\n",
        "# посчитать значение метрики accuracy и F1-score\n",
        "y_pred = sigmoid(X, theta) > 0.5\n",
        "print_logisitc_metrics(y, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-omwEW8DAIXi"
      },
      "source": [
        "# Разбить выборку на train/valid, оптимизировать theta,\n",
        "# сделать предсказания и посчитать ошибку F1-score\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\n",
        "theta = optimize(X_train, y_train, calc_binary_cross_entropy_grad, np.ones(m), 1., 300)\n",
        "y_pred = sigmoid(X_valid, theta) > 0.5\n",
        "\n",
        "print_logisitc_metrics(y_valid, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-OQLq-rAIXi"
      },
      "source": [
        "# Отрисовать ROC кривую\n",
        "def calc_and_plot_roc(y_true, y_pred_proba):\n",
        "    # Посчитать значения ROC кривой и значение площади под кривой AUC\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)\n",
        "    roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
        "    \n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
        "    plt.title('Receiver Operating Characteristic', fontsize=15)\n",
        "    plt.xlabel('False positive rate (FPR)', fontsize=15)\n",
        "    plt.ylabel('True positive rate (TPR)', fontsize=15)\n",
        "    plt.legend(fontsize=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxnrIy4PAIXj"
      },
      "source": [
        "# Вычислить вероятности принадлежности классу 1 для каждого объекта из валидационной выборки\n",
        "y_pred_proba = sigmoid(X_valid, theta)\n",
        "calc_and_plot_roc(y_valid, y_pred_proba)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYeBsumkAIXj"
      },
      "source": [
        "## 4. Добавление регуляризации"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtdqmYPXAIXj"
      },
      "source": [
        "### 4.1. Оборачивание линейной регрессии в класс"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzm3nYf4AIXj"
      },
      "source": [
        "class RegOptimizer():\n",
        "    def __init__(self, alpha, n_iters):\n",
        "        self.theta = None\n",
        "        self._alpha = alpha\n",
        "        self._n_iters = n_iters\n",
        "    \n",
        "    def gradient_step(self, theta, theta_grad):\n",
        "        return theta - self._alpha * theta_grad\n",
        "    \n",
        "    def grad_func(self, X, y, theta):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def optimize(self, X, y, start_theta, n_iters):\n",
        "        theta = start_theta.copy()\n",
        "\n",
        "        for _ in range(n_iters):\n",
        "            theta_grad = self.grad_func(X, y, theta)\n",
        "            theta = self.gradient_step(theta, theta_grad)\n",
        "\n",
        "        return theta\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        m = X.shape[1]\n",
        "        start_theta = np.ones(m)\n",
        "        self.theta = self.optimize(X, y, start_theta, self._n_iters)\n",
        "        \n",
        "    def predict(self, X):\n",
        "        raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edhpp65mAIXk"
      },
      "source": [
        "class LinReg(RegOptimizer):\n",
        "    def grad_func(self, X, y, theta):\n",
        "        n = X.shape[0]\n",
        "        grad = 1. / n * X.transpose().dot(X.dot(theta) - y)\n",
        "\n",
        "        return grad\n",
        "    \n",
        "    def predict(self, X):\n",
        "        if self.theta is None:\n",
        "            raise Exception('You should train the model first')\n",
        "        \n",
        "        y_pred = X.dot(self.theta)\n",
        "        \n",
        "        return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNRiFuQiAIXk"
      },
      "source": [
        "def print_regression_metrics(y_true, y_pred):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    print(f'MSE = {mse:.2f}, RMSE = {rmse:.2f}')\n",
        "def prepare_boston_data():\n",
        "    data = load_boston()\n",
        "    X, y = data['data'], data['target']\n",
        "    # Нормализовать даннные с помощью стандартной нормализации\n",
        "    X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "    # Добавить фиктивный столбец единиц (bias линейной модели)\n",
        "    X = np.hstack([np.ones(X.shape[0])[:, np.newaxis], X])\n",
        "    \n",
        "    return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAvEaFBZAIXl"
      },
      "source": [
        "linreg = LinReg(0.01, 500)\n",
        "X, y = prepare_boston_data()\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HD7ugBp_AIXl"
      },
      "source": [
        "linreg.fit(X_train, y_train)\n",
        "y_pred = linreg.predict(X_valid)\n",
        "print_regression_metrics(y_valid, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-RO6Y_nAIXl"
      },
      "source": [
        "### 4.2. Оборачивание логистической регрессии в класс"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKSNXL2IAIXm"
      },
      "source": [
        "class LogReg(RegOptimizer):\n",
        "    def sigmoid(self, X, theta):\n",
        "        return 1. / (1. + np.exp(-X.dot(theta)))\n",
        "    \n",
        "    def grad_func(self, X, y, theta):\n",
        "        n = X.shape[0]\n",
        "        grad = 1. / n * X.transpose().dot(self.sigmoid(X, theta) - y)\n",
        "\n",
        "        return grad\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        return self.sigmoid(X, self.theta)\n",
        "    \n",
        "    def predict(self, X):\n",
        "        if self.theta is None:\n",
        "            raise Exception('You should train the model first')\n",
        "        \n",
        "        y_pred = self.predict_proba(X) > 0.5\n",
        "        \n",
        "        return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxF63u4wAIXm"
      },
      "source": [
        "def prepare_adult_data():\n",
        "    adult = pd.read_csv('./data/adult.data',\n",
        "                        names=['age', 'workclass', 'fnlwgt', 'education',\n",
        "                               'education-num', 'marital-status', 'occupation',\n",
        "                               'relationship', 'race', 'sex', 'capital-gain',\n",
        "                               'capital-loss', 'hours-per-week', 'native-country', 'salary'])\n",
        "    \n",
        "    # Избавиться от лишних признаков\n",
        "    adult.drop(['native-country'], axis=1, inplace=True)\n",
        "    # Сконвертировать целевой столбец в бинарные значения\n",
        "    adult['salary'] = (adult['salary'] != ' <=50K').astype('int32')\n",
        "    # Сделать one-hot encoding для некоторых признаков\n",
        "    adult = pd.get_dummies(adult, columns=['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex'])\n",
        "    \n",
        "    # Нормализовать нуждающиеся в этом признаки\n",
        "    a_features = adult[['age', 'education-num', 'hours-per-week', 'fnlwgt', 'capital-gain', 'capital-loss']].values\n",
        "    norm_features = (a_features - a_features.mean(axis=0)) / a_features.std(axis=0)\n",
        "    adult.loc[:, ['age', 'education-num', 'hours-per-week', 'fnlwgt', 'capital-gain', 'capital-loss']] = norm_features\n",
        "    \n",
        "    # Разбить таблицу данных на матрицы X и y\n",
        "    X = adult[list(set(adult.columns) - set(['salary']))].values\n",
        "    y = adult['salary'].values\n",
        "\n",
        "    # Добавить фиктивный столбец единиц (bias линейной модели)\n",
        "    X = np.hstack([np.ones(X.shape[0])[:, np.newaxis], X])\n",
        "    \n",
        "    return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GA5vWunAIXm"
      },
      "source": [
        "logreg = LogReg(1., 300)\n",
        "X, y = prepare_adult_data()\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBO1r8xxAIXn"
      },
      "source": [
        "# Разбить выборку на train/valid, оптимизировать theta,\n",
        "# сделать предсказания и посчитать ошибку F1-score\n",
        "\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred = logreg.predict(X_valid)\n",
        "\n",
        "print_logisitc_metrics(y_valid, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHovbVSZAIXn"
      },
      "source": [
        "y_pred_proba = logreg.predict_proba(X_valid)\n",
        "calc_and_plot_roc(y_valid, y_pred_proba)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD5DCTRxAIXn"
      },
      "source": [
        "В случаях линейной и логистической регрессии будем добавлять к функции ошибки регуляризующую часть как:\n",
        "$$\\frac{\\lambda}{2m}\\sum_{j}^{m}{\\theta_j^2},$$\n",
        "где $\\theta$ — вектор параметров линейной модели без фиктивного признака (intercept/bias term), $m$ — количество нефиктивных признаков, $\\lambda$ — параметр регуляризации."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d4gu7IuAIXo"
      },
      "source": [
        "### 4.3. Добавление регуляризатора в линейную регрессию"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rq47iOvIAIXo"
      },
      "source": [
        "После добавления регуляризации функция ошибки линейной регрессии будет выглядеть следующим образом:\n",
        "$$L=\\frac{1}{2n} * \\sum_{i=1}^{n}{(y_i - \\theta^Tx_i)^2} + \\frac{\\lambda}{2m}\\sum_{j}^{m}{\\theta_j^2}$$\n",
        "А ее градиент по параметру $\\theta$:\n",
        "$$\\nabla L = \\frac{1}{n}\\sum_{i=1}^{n}{(\\theta^Tx_i - y_i) \\cdot x_i} + \\frac{\\lambda}{m}\\sum_{j=1}^{m}{\\theta_j} = \\frac{1}{n}X^T(X\\theta - y) + \\frac{\\lambda}{m}\\sum_{j=1}^{m}{\\theta_j}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-CcE58SAIXo"
      },
      "source": [
        "class LinRegRegularized(LinReg):\n",
        "    def __init__(self, alpha, lambd, n_iters):\n",
        "        super(LinRegRegularized, self).__init__(alpha, n_iters)\n",
        "        self._lambd = lambd\n",
        "    \n",
        "    def grad_func(self, X, y, theta):\n",
        "        n = X.shape[0]\n",
        "        grad = 1. / n * X.transpose().dot(X.dot(theta) - y)\n",
        "        grad_term = self._lambd * np.mean(theta)\n",
        "\n",
        "        return grad + grad_term"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzWjBpgNAIXp"
      },
      "source": [
        "linreg = LinRegRegularized(alpha=0.01, lambd=0.05, n_iters=500)\n",
        "X, y = prepare_boston_data()\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShGdk8vLAIXp"
      },
      "source": [
        "linreg.fit(X_train, y_train)\n",
        "y_pred = linreg.predict(X_valid)\n",
        "print_regression_metrics(y_valid, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v7zYhJfAIXp"
      },
      "source": [
        "### 4.4. Добавление регуляризатора в логистическую регрессию"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMA2dhstAIXp"
      },
      "source": [
        "Функция ошибки для логистической регрессии в случае бинарной классификации с регуляризатором записывается следующим образом:\n",
        "$$L=-\\frac{1}{n}(y_i \\log h_{\\theta}(x_i) + (1-y_i) \\log(1-h_{\\theta}(x_i)))+\\frac{\\lambda}{2m}\\sum_{j}^{m}{\\theta_j^2},$$\n",
        "где $x_i$ — вектор признаков $i$-го примера из обучающей выборки, $y_i$ — истинный класс для соответствующего примера (0 или 1), $n$ — число примеров в обучающей выборке, $m$ — количество нефиктивных признаков, $\\lambda$ — параметр регуляризации, $h_{\\theta}(x)$ — sigmoid функция, равная:\n",
        "$$h_{\\theta}(x)=\\frac{1}{1+\\exp^{-\\theta x}},$$\n",
        "где $\\theta$ — вектор параметров логистической регрессии, $x$ - вектор признаков объекта из выборки."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUTnIUxJAIXq"
      },
      "source": [
        "Соответствующий градиент функции ошибки равен:\n",
        "$$\\nabla L=\\frac{1}{n}\\sum_{i=1}^{n}{(h_{\\theta}(x_i)-y_i)x_i}+\\frac{\\lambda}{m}\\sum_{j}^{m}{\\theta_j}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cg6jlG5XAIXq"
      },
      "source": [
        "class LogRegRegularized(LogReg):\n",
        "    def __init__(self, alpha, lambd, n_iters):\n",
        "        super(LogRegRegularized, self).__init__(alpha, n_iters)\n",
        "        self._lambd = lambd\n",
        "    \n",
        "    def grad_func(self, X, y, theta):\n",
        "        n = X.shape[0]\n",
        "        grad = 1. / n * X.transpose().dot(self.sigmoid(X, theta) - y)\n",
        "        grad_term = self._lambd * np.mean(theta)\n",
        "\n",
        "        return grad + grad_term"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AverRBqAIXq"
      },
      "source": [
        "logreg = LogRegRegularized(alpha=1., lambd=1., n_iters=300)\n",
        "X, y = prepare_adult_data()\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znEtf2MtAIXq"
      },
      "source": [
        "# Разбить выборку на train/valid, оптимизировать theta,\n",
        "# сделать предсказания и посчитать ошибку F1-score\n",
        "\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred = logreg.predict(X_valid)\n",
        "\n",
        "print_logisitc_metrics(y_valid, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "USs8q0BsAIXr"
      },
      "source": [
        "y_pred_proba = logreg.predict_proba(X_valid)\n",
        "calc_and_plot_roc(y_valid, y_pred_proba)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXxGvqInAIXr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}